{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanklin3/18065/blob/main/6_7920_Fall_2024_Homework_5_problem_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n"
      ],
      "metadata": {
        "id": "wM6ks-aGKjJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this problem, we will consider the discounted stochastic LQ control problem, as described in Lecture 3. Specifically, we will consider using Q-learning to approximate the Q-function of this problem. Below, we provide an instant of this problem through the environment **LQEnv**. The environment is a python class, whose interface is very similar to that of OpenAI gym."
      ],
      "metadata": {
        "id": "fXiBmxYfofy3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code block below imports important and relevant packages, defnies the environment, and includes helper functions like a policy evaluation function."
      ],
      "metadata": {
        "id": "TSSjTKeTn29X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import gym\n",
        "from gym import error, spaces, utils\n",
        "from gym.utils import seeding\n",
        "\n",
        "dtype = np.float32\n",
        "class LQEnv(gym.Env):\n",
        "  def __init__(self, n, m, seed = 0, gamma = 0.9, sigma = 0.2):\n",
        "    \"\"\"\n",
        "    The LQ environment, initialized with the parameters\n",
        "    n: state dimension\n",
        "    m: action dimension\n",
        "    seed: random seed\n",
        "    gamma: discount factor\n",
        "    sigma: noise variance\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    self.n = n\n",
        "    self.action_space = spaces.Box(low = -np.inf, high = np.inf, shape =  [m,1], dtype = dtype )\n",
        "    self.observation_space = spaces.Box(low = -np.inf, high = np.inf, shape =  [n,1], dtype = dtype )\n",
        "    self.A = np.array([[0.0488135 , 0.21518937],\n",
        "                       [0.10276338, 0.04488318]\n",
        "                        ])\n",
        "    self.B =  np.array([[-0.0763452 ,  0.14589411],\n",
        "                        [-0.06241279,  0.391773  ]\n",
        "                       ])\n",
        "    self.R =  np.array([[1.57567331, 0.96575621],\n",
        "                        [0.96575621, 1.40655837]\n",
        "                        ])\n",
        "    self.Q =  np.array([[1.67940376, 0.12099823],\n",
        "                        [0.12099823, 0.51263764]\n",
        "                        ])\n",
        "    self.state = self.observation_space.sample()\n",
        "    self.sigma = sigma\n",
        "    self.gamma = gamma\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    This method is the primary interface between environment and agent.\n",
        "    Paramters:\n",
        "        action: array of shape [m,1]\n",
        "\n",
        "    Returns:\n",
        "        output: (next state:array, cost:float, done:bool, None)\n",
        "\n",
        "    \"\"\"\n",
        "    err_msg = f\"{action!r} ({type(action)}) invalid\"\n",
        "    assert self.action_space.contains(action), err_msg\n",
        "    assert self.state is not None, \"Call reset before using step method.\"\n",
        "    w = self.sigma*np.random.randn()\n",
        "\n",
        "    cost = (self.state.T.dot(self.Q)).dot(self.state) +  (action.T.dot(self.R)).dot(action)\n",
        "\n",
        "    self.state = self.A.dot(self.state) + self.B.dot(action) + w\n",
        "\n",
        "    done = False\n",
        "\n",
        "    return self.state, cost, done, {}\n",
        "\n",
        "  def reset(self, state = None):\n",
        "    \"\"\"\n",
        "    This method resets the environment to its initial values.\n",
        "    Paramters:\n",
        "        state array\n",
        "            set the env to specifc state (optional)\n",
        "    Returns:\n",
        "        observation:    array\n",
        "                        the initial state of the environment\n",
        "    \"\"\"\n",
        "    if state is None:\n",
        "        # sample from a guassian with zero mean and std of 10\n",
        "        self.state = 10*self.observation_space.sample()\n",
        "    else:\n",
        "        self.state = state\n",
        "    return  self.state\n",
        "\n",
        "  def close(self):\n",
        "    \"\"\"\n",
        "    This method provides the user with the option to perform any necessary cleanup.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "  def sample_random_action(self):\n",
        "      \"\"\"\n",
        "      sample actions from a normal dist with mean zero and std 10\n",
        "      \"\"\"\n",
        "      return  10*env.action_space.sample()\n",
        "\n",
        "\n",
        "def policy_evaluation(policy, env, T = 1000, N = 5):\n",
        "    \"\"\"\n",
        "    This method evaluate the performance of a specifc policy on the env through simulating\n",
        "    it on N trajectories each of length T.\n",
        "    Paramters:\n",
        "        policy: function that takes in state and return action\n",
        "        env: instant of LQEnv\n",
        "        T: number of iterations per trajectory\n",
        "        N: number of trajectories\n",
        "    Returns:\n",
        "        output: mean discounted total cost\n",
        "\n",
        "    \"\"\"\n",
        "    costs = []\n",
        "    for ite in tqdm(range(N)):\n",
        "        state = env.reset()\n",
        "        gamma = env.gamma\n",
        "        total_costs = 0\n",
        "        for t in range(T):\n",
        "            action = policy(state)\n",
        "            state, cost, _, _ = env.step(action)\n",
        "            total_costs += cost * gamma**(t)\n",
        "        costs.append(total_costs)\n",
        "    return np.mean(costs)\n",
        "\n",
        "def lin_policy(K):\n",
        "    \"\"\"\n",
        "    helper function to define linear policies of the form u = L@x\n",
        "    \"\"\"\n",
        "    def policy(state):\n",
        "        return K.dot(state)\n",
        "    return policy\n",
        "\n",
        "\n",
        "def Q_a(x,u, theta):\n",
        "    \"\"\"\n",
        "    Q-function parameterized by the tuple/list theta\n",
        "    \"\"\"\n",
        "    q =  x.T@theta[0].T@theta[0]@x\n",
        "    q+= u.T@theta[1].T@theta[1]@u\n",
        "    q+= 2*x.T@theta[2]@u + theta[3]\n",
        "    return q[0,0]\n",
        "\n"
      ],
      "metadata": {
        "id": "UAiHtkQUKpsv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize environemnt\n",
        "\n",
        "Below, we initialize the environment that you need to interact with in this excercise."
      ],
      "metadata": {
        "id": "LmdZmBU4qwcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 2\n",
        "m = 2\n",
        "env = LQEnv(n,m)"
      ],
      "metadata": {
        "id": "CE8fn-nEqvVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (a) Assuming knowledge of the system matrices $A, B, R,$ and $Q$, compute $P$, the solution to the appropriate form of the Riccati equation, and the matrix $K$ that characterizes the optimal policy. Evaluate the policy performance over 100 trajectories each with length 1000.\n"
      ],
      "metadata": {
        "id": "VwnEuE3Jqmtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Your code goes here\n"
      ],
      "metadata": {
        "id": "31jSaZK4Kov0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (b) Given an arbitrary $\\Theta_0$, write the greedy policy with respect to $Q(x; u;â€€\\Theta_0)$ in terms of the parameters $\\Theta_0$. What is the form of this greedy policy?"
      ],
      "metadata": {
        "id": "9LZ9Z16I3vPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*type your answer here*"
      ],
      "metadata": {
        "id": "Bf912Htz39Df"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (c) Implement Q-learning for this problem, where actions are chosen using the greedy policy (wrt to the current Q-function). Then train using samples chosen from a single arbitrarily long trajectory. Use the given function to initialize theta.\n",
        "\n",
        "Hint: If your states/parameters grow very large during training, consider making the learning rate very small"
      ],
      "metadata": {
        "id": "d38u384zrqVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_theta(n,m, fun = np.random.randn, seed = 1):\n",
        "    np.random.seed(seed)\n",
        "    A = fun(n,n)\n",
        "    B = fun(m,m)\n",
        "    C = fun(n,m)\n",
        "    const = 0\n",
        "    return [A,B,C, const]\n",
        "\n"
      ],
      "metadata": {
        "id": "R5gRkrXOaEWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Code goes HERE"
      ],
      "metadata": {
        "id": "Arp3q5lNQNeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Do the parameters converge? Is the resulting policy the same as the policy obtained in part (a)? If not, why?\n"
      ],
      "metadata": {
        "id": "hGXFkSHExnoP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Type your answer here*"
      ],
      "metadata": {
        "id": "5ojzzW6DxscC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (d) Train using  samples chosen from multiple trajectories. Each with length 25.  Also choose actions using an $\\epsilon$-greedy policy for $\\epsilon = 0.3$. Do the parameters converge? Is the resulting policy the same as the policy obtained in part (a)? Evaluate the policy performance over 100 trajectories each with length 1000 and compare its average cost with that of the optimal policy.\n",
        "\n",
        "Hint: to sample actions randomly, use the method: env.sample_random_action()"
      ],
      "metadata": {
        "id": "Fhov62jOz28r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here"
      ],
      "metadata": {
        "id": "YVVYO38vXV7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Do the parameters converge? Is the resulting policy the same as the policy obtained in part (a)?\n"
      ],
      "metadata": {
        "id": "IL8opJc258Q_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*type your answer here*"
      ],
      "metadata": {
        "id": "L7wufXko58R3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the policy performance over 100 trajectories each with length 1000 and compare its average cost with that of the optimal policy.\n"
      ],
      "metadata": {
        "id": "_IHOfPIP29R8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here"
      ],
      "metadata": {
        "id": "jO3BzX7h47aC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (e)  Try different values of epsilons $\\in \\{0.2,0.4,0.6, 0.8\\}$ (using the same initialization of $\\Theta$). With which value of epsilon does the algorithm converges faster?\n",
        "\n"
      ],
      "metadata": {
        "id": "EsCPN4i8-D1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here"
      ],
      "metadata": {
        "id": "0okkkNdLjjV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  (f) Is it fair to generalize the conclusions we have found about the optimal way to select epsilon and the trajectory length to other problems/environment? For example, if you have found that a single infinite trajectory does not work well, and larger values of epsilon works better, can you generalize this to other environments/problems like, for example, chess? why/why not?"
      ],
      "metadata": {
        "id": "s9kOnCafCO1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*type your answer here*"
      ],
      "metadata": {
        "id": "LFKqS71bXj28"
      }
    }
  ]
}